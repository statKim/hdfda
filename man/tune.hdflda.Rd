% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hdflda.R
\name{tune.hdflda}
\alias{tune.hdflda}
\title{Hyperparameter tuning for \code{hdflda}}
\usage{
tune.hdflda(
  X,
  y,
  tune_method = "abic",
  grid = NULL,
  basis = "bspline",
  n_basis_list = NULL,
  lambda_list = NULL,
  measure = "accuracy",
  tol = 1e-07,
  K = 5,
  tie_break = "sparse"
)
}
\arguments{
\item{X}{a n-m-p array (p-variate functional data; each functional data consists of n curves observed from m timepoints)}

\item{y}{a integer vector containing class label of X (n x 1 vector)}

\item{tune_method}{"cv", "abic" or "abic-aic" (default is "cv")}

\item{grid}{a vector containing m timepoints}

\item{basis}{"bspline" is only supported}

\item{n_basis_list}{a vector containing the candidate of \code{n_basis} (the number of cubic B-spline bases using \code{n_basis}-2 knots)}

\item{lambda_list}{a vector containing the candidate of \code{lambda} (a penalty parameter for L1-regularization)}

\item{measure}{the loss function for the cross-validation. "accuracy" or "cross.entropy" (Default is "accuracy")}

\item{tol}{a tolerance rate to define the sparse discriminant set}

\item{K}{the nuber of folds for K-fold CV}

\item{tie_break}{the tie breaking rule for cross-validation. "sparse"(default) choose the largest \code{lambda} and the smallest \code{n_basis}; "random" choose randomly}
}
\value{
a \code{hdflda} object
}
\description{
Select the optimal \code{n_basis} and \code{lambda} for \code{hdflda} using ABIC or K-fold cross-validation
Parallel computing can be used by using the \code{doParallel} package usages.
}
\examples{
\dontrun{
library(doParallel)
cl <- makePSOCKcluster(detectCores()/2)
registerDoParallel(cl)
fit <- tune.hdflda(X_train, y_train)
stopCluster(cl)
pred <- predict(fit$opt_fit, X_test)
mean(pred != y_test)
}

}
